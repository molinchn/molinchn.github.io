(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{372:function(_,t,a){"use strict";a.r(t);var v=a(4),r=Object(v.a)({},(function(){var _=this,t=_._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[t("p",[_._v("[TOC]")]),_._v(" "),t("h2",{attrs:{id:"_1-循环神经网络rnn"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-循环神经网络rnn"}},[_._v("#")]),_._v(" 1. 循环神经网络RNN")]),_._v(" "),t("h3",{attrs:{id:"_1-1-网络结构图"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-网络结构图"}},[_._v("#")]),_._v(" 1.1 网络结构图")]),_._v(" "),t("p",[t("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123150405734.png",alt:"image-20201123150405734"}})]),_._v(" "),t("p",[_._v("以右侧$x_t$所在的一列为特例说明符号：")]),_._v(" "),t("ul",[t("li",[_._v("$x_t$ 输入向量")]),_._v(" "),t("li",[_._v("$U$ 输入层到隐藏层的权重矩阵")]),_._v(" "),t("li",[_._v("$s_{t}$ 隐藏层向量，根据计算公式有$s_t = f(Ux_t + Ws_{t-1})$，其中$f(*)$是输入层到隐藏层的激活函数")]),_._v(" "),t("li",[_._v("$V$ 是隐藏层到输出层的权重矩阵")]),_._v(" "),t("li",[_._v("$o_t$ 是输出向量，且有$o_t = g(Vs_t)$")]),_._v(" "),t("li",[_._v("$W$ 是循环神经网络特有的权重矩阵，也是连接所有独立列的关键。下面会着重推一下$W$是如何连接起整个循环神经网络的")])]),_._v(" "),t("h3",{attrs:{id:"_1-2-w-是如何连接循环神经网络的"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-w-是如何连接循环神经网络的"}},[_._v("#")]),_._v(" 1.2 $W$是如何连接循环神经网络的")]),_._v(" "),t("p",[_._v("仍然以 $x_t$ 和 $o_t$ 所在的一列作为例子说明。")]),_._v(" "),t("p",[_._v("由上述的神经网络可以得到：$o_t = g(Vs_t)$")]),_._v(" "),t("p",[_._v("且有$s_t = f(Ux_t + Ws_{t-1})$,因此可以带入上面的$o_t$表达式开始套娃：\n$$\no_t = g(Vs_t) \\\n= g(Vf(Ux_t + Ws_{t-1}))\\\n= g(Vf(Ux_t + Wf(Ux_{t-1} + Ws_{t-2})))\\\n= g(Vf(Ux_t + Wf(Ux_{t-1} + Wf(Ux_{t-2} + Ws_{t-3})))\\\n=……………………………………………\n$$\n这样想要计算$t$时刻的$o_t$，就需要使用前面所有时刻的输入。")]),_._v(" "),t("p",[t("strong",[_._v("换言之，$o_t$中包含了前面所有输入的信息")])]),_._v(" "),t("h2",{attrs:{id:"_2-双向循环神经网络"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-双向循环神经网络"}},[_._v("#")]),_._v(" 2. 双向循环神经网络")]),_._v(" "),t("p",[_._v("虽然普通的RNN包含了前面所有的输入，但是有时候我们还需要后面未来的输入来确定当前的输出，比如[参考博客]("),t("a",{attrs:{href:"https://zybuluo.com/hanbingtao/note/541458#%E5%9F%BA%E6%9C%AC%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C",target:"_blank",rel:"noopener noreferrer"}},[_._v("零基础入门深度学习(5) - 循环神经网络 - 作业部落 Cmd Markdown 编辑阅读器 (zybuluo.com)"),t("OutboundLink")],1),_._v(")中的一个例子：")]),_._v(" "),t("blockquote",[t("p",[_._v("我的手机坏了，我打算____一部新手机。")])]),_._v(" "),t("p",[_._v("如果需要用一个RNN来填词，那必须要考虑后面的输入，才能确定当前的输出。")]),_._v(" "),t("p",[_._v("双向RNN的网路结构与单向有很大的相似之处，其实就是将两个方向相反的普通RNN叠加")]),_._v(" "),t("h3",{attrs:{id:"_2-1-网络结构"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-网络结构"}},[_._v("#")]),_._v(" 2.1 网络结构")]),_._v(" "),t("p",[t("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123152740181.png",alt:"image-20201123152740181"}})]),_._v(" "),t("p",[_._v("这里没有列出各个权重矩阵，但是它们都是存在的，而且和刚才一致。")]),_._v(" "),t("p",[_._v("这里以$x1$输入$y_1$输出的一列作为举例：")]),_._v(" "),t("p",[_._v("显然我们有$y_1 = g(VA_1 + V^{'}A_1^{'})$，其中$V$和$V^{'}$都是权重矩阵")]),_._v(" "),t("p",[_._v("这时候避免混乱开始分开写$A_1$和$A_2$的表达式")]),_._v(" "),t("p",[_._v("$A_1 = f(WA_0 + Ux_1)$")]),_._v(" "),t("p",[_._v("$A_1^{'} = f(WA_2^{'} + Ux_1)$")]),_._v(" "),t("p",[_._v("上面两个公式都是带入$y_1$的表达式。秉着套娃的精神其实还可以继续写$A_0$和$A_2^{'}$但是已经没必要了，可以看出，两个$A$一个往前套娃，一个往后套娃，这样就获得了前面所有的信息和后面所有的信息，刚才所提出的**”同时需要过去和未来输入来决定这一时刻的输出“**的目的也就达到了。")]),_._v(" "),t("h2",{attrs:{id:"_3-深度循环神经网络"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-深度循环神经网络"}},[_._v("#")]),_._v(" 3. 深度循环神经网络")]),_._v(" "),t("p",[_._v("挖个坑，参考的博客写的比较简单，日后待补充")]),_._v(" "),t("h2",{attrs:{id:"_4-利用bptt算法训练"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-利用bptt算法训练"}},[_._v("#")]),_._v(" 4. 利用BPTT算法训练")]),_._v(" "),t("p",[_._v("挖坑，待填")]),_._v(" "),t("h2",{attrs:{id:"_5-rnn中的梯度爆炸与消失"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-rnn中的梯度爆炸与消失"}},[_._v("#")]),_._v(" 5. RNN中的梯度爆炸与消失")]),_._v(" "),t("h3",{attrs:{id:"_5-1-rnn中梯度爆炸和消失的原因"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-rnn中梯度爆炸和消失的原因"}},[_._v("#")]),_._v(" 5.1 RNN中梯度爆炸和消失的原因")]),_._v(" "),t("h3",{attrs:{id:"_5-2-应对措施"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-应对措施"}},[_._v("#")]),_._v(" 5.2 应对措施")]),_._v(" "),t("p",[_._v("针对梯度爆炸，可以设置阈值截取梯度")]),_._v(" "),t("p",[_._v("针对梯度消失比较难处理，方法有三类：")]),_._v(" "),t("ol",[t("li",[_._v("合理初始化权重值，避开梯度消失的区域。")]),_._v(" "),t("li",[_._v("用ReLU代替sigmoid和tanh作为激活函数")]),_._v(" "),t("li",[_._v("使用其他结构的RNN，例如LSTM和GRU")])]),_._v(" "),t("h2",{attrs:{id:"参考"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[_._v("#")]),_._v(" 参考")]),_._v(" "),t("p",[_._v("[零基础入门深度学习(5) - 循环神经网络]("),t("a",{attrs:{href:"https://zybuluo.com/hanbingtao/note/541458#%E5%9F%BA%E6%9C%AC%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C",target:"_blank",rel:"noopener noreferrer"}},[_._v("零基础入门深度学习(5) - 循环神经网络 - 作业部落 Cmd Markdown 编辑阅读器 (zybuluo.com)"),t("OutboundLink")],1),_._v(")")])])}),[],!1,null,null,null);t.default=r.exports}}]);