(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{369:function(t,a,_){"use strict";_.r(a);var r=_(4),s=Object(r.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"_1-lstm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-lstm"}},[t._v("#")]),t._v(" 1. LSTM")]),t._v(" "),a("h3",{attrs:{id:"_1-1-网络的大致结构与设计思路"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-网络的大致结构与设计思路"}},[t._v("#")]),t._v(" 1.1 网络的大致结构与设计思路")]),t._v(" "),a("p",[t._v("循环神经网络的反向传播公式为\n$$\n\\delta_k^T = \\delta_k^T \\prod_{i=k}^{t-1}diag[f^{'}(net_i)]W\n$$\n当$\\delta_k^T$从t时刻传播到k时刻，$\\delta_k^T$的上界可以表示为：\n$$\n|\\delta_k^T| \\leq |\\delta_k^T| \\prod_{i=k}^{t-1}|diag[f^{'}(net_i)]| |W| \\\n\\leq |\\delta_k^T|(\\beta_f\\beta_W)^{t-k}\n$$\n从上式可以看出，当$t-k$非常大时（长时间跨度），$\\beta_f\\beta_W$必须尽可能贴近于1，如果小于1就会出现梯度消失。")]),t._v(" "),a("p",[t._v("当出现梯度消失时，反向传播过程中的误差就无法长距离的传递，这回对网络训练带来极大的难度。")]),t._v(" "),a("p",[t._v("为了解决这个问题，才出现了LSTM。")]),t._v(" "),a("blockquote",[a("p",[t._v("LSTM解决梯度消失的思路就是：增加一个状态c来保存长期的状态，并设计一个控制c的方法。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123193543138.png",alt:"image-20201123193543138"}})])]),t._v(" "),a("p",[t._v("大致的结构就是下面这样：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123193552841.png",alt:"image-20201123193552841"}})]),t._v(" "),a("p",[t._v("符号说明：")]),t._v(" "),a("ul",[a("li",[t._v("$x_t$输入向量")]),t._v(" "),a("li",[t._v("$h_t$短期状态")]),t._v(" "),a("li",[t._v("$c_t$长期状态")])]),t._v(" "),a("h3",{attrs:{id:"_1-2-网络中-门-的概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-网络中-门-的概念"}},[t._v("#")]),t._v(" 1.2 网络中“门”的概念")]),t._v(" "),a("blockquote",[a("p",[t._v("门实际上就是一层"),a("strong",[t._v("全连接层")]),t._v("，它的输入是一个向量，输出是一个0到1之间的实数向量。即\n$$\ng(x) = \\sigma(Wx + b)\n$$\n其中，$b$是偏置项，$x$是输入，$W$是权重矩阵，$\\sigma(*)$是sigmoid函数，它的值域是(0,1).")])]),t._v(" "),a("h4",{attrs:{id:"_1-2-1-遗忘门、-输入门与输出门"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-1-遗忘门、-输入门与输出门"}},[t._v("#")]),t._v(" 1.2.1 遗忘门、 输入门与输出门")]),t._v(" "),a("p",[t._v("· 称控制长期状态单元的门为"),a("strong",[t._v("遗忘门(forget gate)")]),t._v(",它决定了上一刻的状态单元$c_{t-1}$有多少保留到了当前的$c_t$。")]),t._v(" "),a("p",[t._v("· 称"),a("strong",[t._v("控制$x_t$有多少保存到$c_t$的门")]),t._v("为输入门,")]),t._v(" "),a("p",[t._v("· 称"),a("strong",[t._v("控制$c_t$有多少输出到当前输出值$h_t$的门")]),t._v("为输出门。")]),t._v(" "),a("h3",{attrs:{id:"_1-3-lstm最终的网络结构-前向传播"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-lstm最终的网络结构-前向传播"}},[t._v("#")]),t._v(" 1.3 LSTM最终的网络结构（前向传播）")]),t._v(" "),a("p",[t._v("首先，LSTM是一个循环神经网络，它首先将上次的输出与这次的输入两个向量组合起来，即$[h_{t-1}, x_t]$，作为各个门的输入。")]),t._v(" "),a("h4",{attrs:{id:"_1-3-1-遗忘门的构造"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-1-遗忘门的构造"}},[t._v("#")]),t._v(" 1.3.1 遗忘门的构造")]),t._v(" "),a("p",[t._v("$$\nf_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\n$$")]),t._v(" "),a("p",[t._v("这首先构成了一个上面提到的门，$W_f$是遗忘门的权重矩阵，$b_f$是偏置项，这些在后面的输入门与输出门是类似的含义，不再赘述。此时的结构图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123203449263.png",alt:"image-20201123203449263"}})]),t._v(" "),a("h4",{attrs:{id:"_1-3-2-输入门的构造"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-2-输入门的构造"}},[t._v("#")]),t._v(" 1.3.2 输入门的构造")]),t._v(" "),a("p",[t._v("公式如下：\n$$\ni_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)\n$$\n这里再强调一下，输入门的目的是表达输入$x_t$有多少保存到了长期状态$c_t$中，但是现在还看不出来，因为最终的线没有连上，如下图。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123204905775.png",alt:"image-20201123204905775"}})]),t._v(" "),a("h4",{attrs:{id:"_1-3-3-输入单元的构造"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-3-输入单元的构造"}},[t._v("#")]),t._v(" 1.3.3 输入单元的构造")]),t._v(" "),a("p",[t._v("$$\n\\tilde c_t = tanh(W_c [h_{t-1}, x_t] + b_c)\n$$")]),t._v(" "),a("p",[t._v("输出门想表达的是$c_t$有多少从本层的结果$h_t$中输出了（也暂时看不出来，后面没有连上），如下图。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123204851160.png",alt:"image-20201123204851160"}})]),t._v(" "),a("h4",{attrs:{id:"_1-3-4-c-t-的计算"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-4-c-t-的计算"}},[t._v("#")]),t._v(" 1.3.4 $c_t$的计算")]),t._v(" "),a("p",[t._v("$$\nc_t = f_t \\circ c_{t-1} + i_t \\circ \\tilde c_t\n$$")]),t._v(" "),a("p",[t._v("这里谁是向量？？")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123205832618.png",alt:"image-20201123205832618"}})]),t._v(" "),a("h4",{attrs:{id:"_1-3-5-输出门的构造"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-5-输出门的构造"}},[t._v("#")]),t._v(" 1.3.5 输出门的构造")]),t._v(" "),a("p",[t._v("$$\no_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)\n$$")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123210147169.png",alt:"image-20201123210147169"}})]),t._v(" "),a("h4",{attrs:{id:"_1-3-6-最终输出"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-6-最终输出"}},[t._v("#")]),t._v(" 1.3.6 最终输出")]),t._v(" "),a("p",[t._v("$$\n\\mathrm{h}_t = \\mathbf{o}_t\\circ \\tanh(\\mathbf{c}_t)\n$$")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://murray-pic-1254317211.cos.ap-guangzhou.myqcloud.com/gitee_bak/image-20201123210308460.png",alt:"image-20201123210308460"}})]),t._v(" "),a("h3",{attrs:{id:"_1-4-lstm的训练"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-lstm的训练"}},[t._v("#")]),t._v(" 1.4 LSTM的训练")]),t._v(" "),a("h2",{attrs:{id:"_2-gru"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-gru"}},[t._v("#")]),t._v(" 2. GRU")]),t._v(" "),a("h2",{attrs:{id:"参考"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[t._v("#")]),t._v(" 参考")]),t._v(" "),a("p",[t._v("【1】"),a("a",{attrs:{href:"https://zh.d2l.ai/index.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("《动手学深度学习》：面向中文读者、能运行、可讨论 (d2l.ai)"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("【2】"),a("a",{attrs:{href:"https://zybuluo.com/hanbingtao/note/581764#gru",target:"_blank",rel:"noopener noreferrer"}},[t._v("零基础入门深度学习(6) - 长短时记忆网络(LSTM)"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("【3】"),a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/32481747",target:"_blank",rel:"noopener noreferrer"}},[t._v("人人都能看懂的GRU - 知乎 (zhihu.com)"),a("OutboundLink")],1)])])}),[],!1,null,null,null);a.default=s.exports}}]);